# Hugging Face AI 社区介绍

[觉醒元宇宙AI](https://author.baidu.com/home?from=bjh_article&app_id=1736136898153352)

2022-09-30 17:05陕西

## **01** HuggingFace Hub社区简介

**Hugging Face Hub和 Github 类似都是社区**。

Hugging face 刚开始是纽约的聊天机器人初创服务商，他们本来打算用聊天机器人来创业，后来在Github上开源了一个Transformers库，从此这个库在机器学习领域迅速大火起来。目前已经共享了超10w个预训练模型，1w个数据集，超过10w个社区成员，超过5,000公司在使用，超过50,000 GitHub stars，变成了**机器学习界的Github**。



![img](https://pics6.baidu.com/feed/a8014c086e061d954d496b4813a502da62d9ca0b.jpeg@f_auto?token=4b3a9320b6a15fce0798914d07e4a563)



**开源库Transfomers是一个ModelHub, 集成了许多模型，并在Pytorch和TensorFlow上做了一层抽象，隐藏了机器学习框架的细节重视易用性**。

借助Transformers库，可以快速运用NLP大模型，例如BERT、GPT、XLNet、T5或DistilBERT，并使用这些模型处理文本。例如，文本分类、信息抽取、自动QA、文本总结、文本生成等。



## **02** HuggingFace Hub发展历程

**Hugging Face Hub社区的特点：**

- 传统公司往往分为科研部门和工程部门。科研部门的任务就是发论文，推动整个领域的发展。而工程部分的任务就是工程化和商业化。科研部门和工程部门相互独立，交流很少，尤其是大公司。
- 系统地整合和复现科研成果，很多公司都在做，但是以工程友好的方式开源出来，并没有人来做。

解决方案：bridges the gap between science and production

- 为了解决中间缺失的一层，系统地把科研结果收集起来，并以工程友好的方式进行整合和开源。
- 科研人员喜欢分享他们的模型、测试其他人的模型、深入了解这些模型的架构内部的工具。
- 为NLP从业者创建一个足够简单的抽象，这样任何 NLP 从业者都可以在科研人员发布这些模型后几个小时内真正使用这些模型。
- 用community-base的方式开发。

过去所有的技术进步，从来都不是仅仅靠一个人、一个公司或一个组织就能实现的，技术进步一直是整个领域协作工作的结果。机器学习更是如此，它是科学驱动的，科研社区是开放和协作的关键领域。

通过尝试建立一个平台而不是试图与其他公司竞争来实现这一点。通过这样做，可以对整个领域产生更大的影响力，因为你与世界各地最好的科学家合作和出色的团队进行了大量合作，这就是Hugging Face将如何实现如此巨大的技术进步。

对于科研人员：把科研成果成果发布到统一的模型库。对于工程人员：快速的把最新的科研成果运营到实践中。过去这么多年，最主要的技术进步就是技术的普及化。机器学习的普及可能是几年来最大的技术进步。

Hugging Face为用户提供了以下主要功能：

- 模型仓库（Model Repository）：Git仓库可以让你管理代码版本、开源代码。而模型仓库可以让你管理模型版本、开源模型等。使用方式与Github类似。
- 模型（Models）：Hugging Face为不同的机器学习任务提供了许多预训练好的机器学习模型供大家使用，这些模型就存储在模型仓库中。
- 数据集（Dataset）：Hugging Face上有许多公开数据集。

**Hugging Face**在NLP领域最出名，其提供的模型大多都是基于Transformer的。为了易用性，Hugging Face还为用户提供了以下几个项目：

- Transformers(github, 官方文档): Transformers提供了上千个预训练好的模型可以用于不同的任务，例如文本领域、音频领域和CV领域。该项目是HuggingFace的核心，可以说学习HuggingFace就是在学习该项目如何使用。

- Datasets(github, 官方文档): 一个轻量级的数据集框架，主要有两个功能：①一行代码下载和预处理常用的公开数据集；

  ② 快速、易用的数据预处理类库。

- Accelerate(github, 官方文档): 帮助Pytorch用户很方便的实现 multi-GPU/TPU/fp16。

- Space(链接)：Space提供了许多好玩的深度学习应用，可以尝试玩一下。

## **03** Hugging Face模型讲解

**1.Transforms简介**

Hugging Face Transformer是Hugging Face最核心的项目，你可以用它做以下事情：

- 直接使用预训练模型进行推理；
- 提供了大量预训练模型可供使用；
- 使用预训练模型进行迁移学；

**2.Transformers安装**

安装Transformers十分简单，在Python命令行中输入下方代码，直接安装即可。

> pip install transformers

**3.使用Transformers进行推理**

如果你的任务比较常见，你可以直接使用Transformer中提供的Pipeline API来解决，使用方法非常简单，直接使用即可。

对于一些特定的任务，没有官方定义的模板，但你也可以直接去官方网站搜索模板，然后显示指定即可。在加载模型时，你可能会因为缺少一些库而报错，这时只需要安装对应的库，然后重启计算机即可。

**4.查找Hugging Face模型**

主要的几个部分：

- 模型介绍（Model Card）: 我们可以通过该文档查看该模型都提供了哪些功能，模型的表现等。

- 模型文件（Files and versions): 从该模块可以下载模型文件，一般包含多种框架的（TF、Pytorch等）模型文件和配置文件等，可以用于离线加载。

- 测试模型(Hosted inference API): 可以直接通过该模块测试自己的模型。同时Hugging Face也提供了Http API可以调用，这样就不需要本地部署了。

  详情请参考：https://huggingface.co/docs/api-inference/index

- 使用该模型的应用（Spaces using …）：这里展示了使用该模型的应用，可以点进去玩一玩。

- 代码样例（Use in Transformers）：你可以通过该模块直接查看该模型的使用方式，直接拷贝代码到项目里就可以用了。

**5.使用Hugging Face模型**

Transformers项目提供了几个简单的API帮助用户使用Hugging Face模型，而这几个简单的API统称为AutoClass(官方文档链接)，包括：

- AutoTokenizer: 用于文本分词
- AutoFeatureExtractor: 用于特征提取
- AutoProcessor: 用于数据处理
- AutoModel: 用于加载模型

通常一个模型会包含上述4个中的部分功能，例如，对于bert-base-uncased模型，就包含“分词”和“模型”两项功能，我们可以通过代码样例（Use in Transformers） 模块查看。

**6.迁移学习**

很多情况下，Hugging Face提供的模型并不能满足我们的需要，所以我们还是要自己训练模型的。此时我们可以使用Hugging Face提供的预训练模型来进行迁移学习，本节将会介绍如何使用Hugging Face进行迁移学习。

使用Hugging Face模型做迁移学习的思路和普通迁移学习几乎一致：

- 首先选择一个和你的任务类似的任务预训练模型，或者直接选择与任务无关的基础模型；
- 从原有模型中拿出主干部分(backbone)；
- 接上自己的下游任务，构建成新的模型；
- 开始训练；

通常HuggingFace模型的的使用都是分两步，首先分词（其他模型可能是特征提取AutoFeatureExtractor等），然后将第一步的结果作为模型的入参。

## **04** Hugging Face数据集

Datasets类库（github, 官方文档）可以让操作者非常方便的访问和分享数据集，也可以用来对NLP、CV、语音等任务进行评价（Evaluation metrics）。Hugging Face数据集的使用。主要分下面几个步骤：

1.安装Datasets类库，直接使用pip安装即可：

> pip install datasets

2.查找数据集

首先，我们需要打开Hugging Face Datasets页面，与Models页面类似，这里展示了Hugging Face的数据集，可以使用标签或名称进行筛选：

我们可以点进我们感兴趣的数据集，查看详情：

Hugging Face的数据集通常包括多个子集(subset)，并且分成了train、validation和test三份。你可以通过预览区域查看你需要的子集。

3.加载数据集

加载Hugging Face只需要用到datasets.load_dataset一个方法就够了。使用方法也很简单，直接填入要加载的数据集就可以了。Hugging Face的数据集都是放在github上的，所以国内估计很难下载成功。这就要用到load_dataset的加载本地数据集。



![img](https://pics5.baidu.com/feed/86d6277f9e2f07088d321e378375b092a901f2aa.png@f_auto?token=1f90101ad37ea07a25413701804ad309)



到这里，数据集入门就讲完了，更多的内容就需要在你有需要的时候自己探索了。

Hugging Face其之所以能够获得如此巨大的成功，在于解决了很多业务和框架之间这一层——每个算法部门都需要一个比算法工程师团队还大的工程团队来做数据采集、 model部署、 对外商业化等工作的问题；也打破了很多算法团队基础组建和工具链不一致的问题，这些都是脏活和累活，所以大家都特别喜欢Hugging Face。

其业务价值也非常大，一方面让我们这些小白，尤其是入门者也能快速用得上科研大牛们训练出的超牛模型。另一方面，这种特别开放的文化和态度，以及利他利己的精神特别吸引人。Hugging Face上面很多业界大牛也在使用和提交新模型，这样虽然没那么多的计算资源和数据集，但不是从头开始，是站在大牛们的肩膀上进行AI工作。

![img](https://pics0.baidu.com/feed/77094b36acaf2eddd2c0eb51ea4109e239019335.png@f_auto?token=ad1eae2352bd13c290e54d753e3d0002)



> 图片来源于网络
>
> 公众号ID：觉醒元宇宙AI



https://baijiahao.baidu.com/s?id=1745384639742885269&wfr=spider&for=pc