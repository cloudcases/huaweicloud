# Hugging Face：史上star增长最快的开源创业公司

[酱油一哥 Warren](https://www.zhihu.com/people/opp-48)

专注AI的VCer

目录

- 愿景
- 主营业务
- 运营数据
- star增长对比
- 和AI Infra创业公司的开源项目对比
- 和非创业公司的AI开源项目相比
- 和Data Infra创业公司的开源项目对比
- 发展历程：为什么想到做Transformers?
- 融资情况
- 团队
- 开源项目成功的原因
- NLP时代背景：BERT和GPT开始席卷NLP
- 公司的具体打法
- 和巨头的竞争
- 商业模式business model
- 发展规划
- 个人总结与思考
- 价值分析
- 开放性的问题

Warren和很多的创业者、AI从业者以及投资人经常提到Hugging Face，但是大家好像都对Hugging Face不太了解，或者对Hugging Face的了解比较片面，所以Warren花了些时间整理了一下Warren对Hugging Face的研究和看法，并写了这篇文章。

通过AI开源创业公司的新物种——Hugging Face，来看软件1.0（逻辑标称）和软件2.0（AI:拟合数据）在开源的区别。



## **愿景**

野心： Hugging Face想成为机器学习界的GitHub



## 主营业务

![img](https://pic1.zhimg.com/80/v2-fa8810aeaa5e6bbfe55fe15e6839af98_1440w.webp)

![img](https://pic2.zhimg.com/80/v2-72524df6c64de2dac66c4a69e8376681_1440w.webp)

Warren这里贴了Transfomers库在GitHub的官方介绍，就不再赘述。

简单来说，**开源库Transfomers，就是一个ModelHub,集成了很多模型，并在TensorFlow和Pytorch上做了一层抽象，屏蔽了机器学习框架的细节，并非常重视易用性**。

**借助 Transformers库，可以快速运用 NLP大模型，例如 BERT、GPT、XLNet、T5 或 DistilBERT，并使用这些模型处理文本**。例如，**文本分类、信息抽取、自动QA、文本总结、文本生成**等。 

对于科研人员：**把科研成果成果发布到统一的模型库**。

对于工程人员：**快速的把最新的科研成果运营到实践中**。



## 运营数据

- 超过100,000 community成员
- 超过15,000 models 和1,400 datasets 被共享
- 超过5,000公司在使用
- 超过50,000 GitHub stars，2019年发布第一个版本。
- 单月pip installs超过100万
- 超过900 contributors



## star增长对比

为了进行横向比较，Warren选取了不同开源项目进行对比。从GitHub Star增长的角度来直观地理解一下Hugging Face。

说明：虽然光看star数量并不能完全反映开源项目质量和经营情况，还要看fork数量、watch数量、contributor数量、commits数量, pull requests数量和下载量等指标。并且还要关注具体项目所做的事和所处的位置，往往**越是偏向于上层的项目Star数量相对较高，越是偏底层的项目Star数量相对较低**。但是Star数量具有一定借鉴意义，**Star数量高的开源项目在一定程度上反映了developer需求的**。



## 和AI Infra创业公司的开源项目对比

![img](https://pic3.zhimg.com/80/v2-4dfcdbd326cbf476dca7114bb37a4452_1440w.webp)



选取了几个AI Infra创业公司的开源项目进行对比，包括弱监督标注工具Snorkel、用于高性能 AI 研究的轻量级 PyTorch wrapper PyTorch Lightning(Grid AI)、开源ChatBot组件Rasa、向量数据库milvus(Zilliz)、**神经搜索框架Jina**、数据应用构建框架Streamlit、机器学习版本控制工具DVC（Iterative）、Feature Store的Feast(Tecton)。

从图中可以看出，无论是在star的绝对数量和增速（斜率）上Hugging Face都是绝对的碾压。



## 和非创业公司的AI开源项目相比

![img](https://pic1.zhimg.com/80/v2-ab287892d69d502c2f5fa328e504fd50_1440w.webp)

选取了几个比较流行的非创业公司的AI开源项目进行对比，包括深度学习框架包括JAX、**Keras**、**PyTorch、Mxnet**、PaddlePaddle(tensorflow star数量确实最高，为了方便观察，所以没有画出来)，**机器学习库Sklearn**和**交互式分析工具Jupyter**。

从图中可以看出，Hugging Face的Transfomers库在star增速上无疑是最快的，在star的绝对数量上Transformers库居然超过了PyTorch，超过Keras指日可待。



## 和Data Infra创业公司的开源项目对比

![img](https://pic3.zhimg.com/80/v2-450d5109fb60fc6df67f2b5bc155fe52_1440w.webp)

选取了几个Data Infra创业公司的开源项目进行对比，从图中可以看出，Hugging Face的transformers库，无论是在star的绝对数量，还是增速（斜率）都是极其出色的。

star的绝对数量超过了Hadoop（Cloudera）、Spark(Databricks)、MongoDB、Kafka(Confluent)、CockroachDB(Cockroach Labs)、TiDB(PingCAP)、redis和couchDB(CouchBase)，按照这个趋势超越Elastic Search指日可待。

其中：

- Databricks(Spark): 380亿美元
- MongoDB：335.6亿美元
- Elastic(Elastic Search)：146.56亿美元
- Confluent(kafka)：107.59亿美元
- Cloudera(Hadoop)：47亿美元
- Pingcap(TiDB)：30亿美元
- Cockroach Labs(cockroachDB)：18亿美元
- Redis Labs(redis)：19亿美元
- CouchBase(CouchDB)：12.29亿美元



## 发展历程：为什么想到做Transformers?

一开始创始人是想做类似Siri的2C chatbot，其中做好NLP是很关键的点。但是做着做着发现难度太大，做不成。

BERT横空出世之后，BERT一开始只有google发布的TensorFlow版，Hugging Face创始人觉得为啥没有Pytorch版本的,后来就实现了**BERT的PyTorch版**，出乎意料的是，竟然得到了开源社区很多人的认可。渐渐地就形成了Transformers库。

这不经让人联想到了Elastic Search的创业初衷，当年Elastic Search创始人Shay Banon还是一个待业工程师，而自己则想为妻子开发一个方便搜索菜谱的应用，所以才接触到 Lucene，后来就有了Elastic Search，创业的idea有时候真的是很偶然:)



## 融资情况

![img](https://pic3.zhimg.com/80/v2-f0303093dd7a644dfb4f582c58e20896_1440w.webp)

融资：Hugging Face总共融了6000多万美元，最新一轮完成了4000万美元的融资。

资金：银行账户上仍有90%的上一轮融的钱没有花。

估值：五倍增长。



**投资机构者：**

Addition、Lux Capital、A.capital、Betaworks、SV angel（GitHub的早期投资机构）

**个人投资者：（大佬是真的多）**

- Dev Ittycheria（MongoDB CEO）
- Florian Douetteau（Dataiku CEO）
- Richard Socher（Salesforce首席科学家 Einstein cloud AI services ）
- Greg Brockman（OpenAI 联合创始人兼CTO）
- Olivier Pomel（Datadog CEO）
- Augusto Marietti(Kong CEO)



## 团队

团队人数：

- 2021年，全职30人，但是contributor 900人（包括谷歌AutoML大神quocle）
- 2020年10月，25人，但是contributor 600人

**Hugging Face虽然只有30个人，但是撬动了整个社区，共有900个contributor**

团队构成：在2021年之前全是工程师

团队分布：大部分法国，少部分纽约。CEO在纽约，CTO在法国

两地各有优势

- 美国：钱、VC、市场、经验
- 法国：顶尖的人才更便宜和稳定



## 开源项目成功的原因

开源项目成功的原因，其实Hugging Face的CEO也没有能完全搞明白。



## NLP时代背景：BERT和GPT开始席卷NLP

- 2018年NLP大突破，e.g., **GPT2 and BERT**，NLP比CV的应用更广泛。
- **模型变化很快，变得越来越复杂，管理和部署的负担越来越重**。
- Huggging Face CEO认为“transfer learning models are starting to eat the whole field of machine learning.” 并且迁移学习开始进入多模态任务。



## 公司的具体打法：

**痛点**：

- 公司往往分为科研部门和工程部门。**科研部门的任务就是发论文，推动整个领域的发展**。而**工程部分的任务就是工程化和商业化**。科研部门和工程部门相互独立，交流很少，尤其是大公司。
- **系统地整合和复现科研成果，很多公司都在做，但是以工程友好的方式开源出来，并没有人来做**。

**解决方案**：bridges the gap between science and production

- 为了解决中间缺失的一层，**系统地把科研结果收集起来，并以工程友好的方式进行整合和开源**。
- 科研人员喜欢分享他们的模型、测试其他人的模型、深入了解这些模型的架构内部的工具。
- 为NLP从业者创建一个足够简单的抽象，这样任何 NLP 从业者都可以在科研人员发布这些模型后几个小时内真正使用这些模型。
- 用community-base的方式开发。

**科学驱动：计算机科学VS机器学习**

- software engineering/ computer science: even if computer science as science in the name of it, it's not a science-driven topic. they don't really read research papers, they don't really follow the science of computer science.
- machine learning: science-driven domain. It all starts from couple of dozen kick-ass kind of NLP science teams all over the world that are creating new models like, BERT, T5, RoBERTa.

**community-base的开发方式的好处？（CEO观点）**

过去所有的技术进步，从来都不是仅仅靠一个人、一个公司或一个组织就能实现的。**技术进步一直是整个领域协作工作的结果**。

机器学习更是如此，因为与机器学习之前构建软件的方式相比，因为**机器学习领域是科学驱动的，科研社区是开放和协作的领域**。

所以通过尝试建立一个平台而不是试图与其他公司竞争来实现这一点。通过这样做，可以对整个领域产生更大的影响力，因为你与世界各地最好的科学家合作和出色的团队进行了大量合作，这就是Hugging Face将如何实现如此巨大的技术进步。

过去这么多年，最主要的技术进步就是技术的平民化。机器学习的民主化可能是几十年来最大的技术进步



**大厂为什么要开源协作？**

FAIR(Facebook)的开源的原因非常清晰，有两点：

- 当做研究时，需要分享、发布、开源、让社区一起，这样才能真正验证或否定、批评或反驳的研究。遵循科学方法，发表论文并能够复现，所以开源代码。实际上最新的例子是：创业公司paper with code是一个很大的支持者，实际上是在收集论文和可复现的实验代码。paper with code实际上是2019年 facebook收购的，只是为了喜欢支持这个并允许他们真正成长并推动他们的可复现性。并且Facebook宣布了与archive的重要合作伙伴关系，这将使论文与代码一起变得非常容易。
- 人工智能的挑战如此之大。仅仅靠自己的力量并不能解决全部问题，所以需要共同解决这些问题。最好的选择实际上是共享代码并使用最开放的许可证，以便其他人可以在其上构建和使用它。也许就会有一个更好的模型回到我们身边。



## 和巨头的竞争

Warren也和国内外大厂的AI部门相关小伙伴探讨过，发现很多人都知道Hugging Face的Tranformers库，也都想过要不要自己也做一个类似的。但是ModelHub需要整个领域科研人员和相关人士的协作（包括各个公司和组织的科研机构），协作意味着需要一定的中立性，如果ModelHub是由一家大厂掌控，一开始就不可能吸引其他的潜在竞争对手的科研部门加入一起贡献模型的。所以一个中立的第三方来做ModelHub就非常合适，而创业公司就是一个好的中立第三方。

Hugging Face全职员工30个，但是contributor有900个，其中包括很多的科研大牛如FAIR的科研人员和google大神Quoc Le。如果是云大厂搞一个类似的ModelHub，结果就是谁也不服谁，其他的云厂商相关人士大概率是不会去贡献model的。所以ModelHub天然就适合一个中立公司牵头搞，撬动整个领域相关人士一起协作。

通过开源，Hugging Face设法聚集了真正强大的社区贡献者，基本上开始成为 NLP的标准，所以现在NLP领域的科研人员有许多会把自己的模型集成到Transformers库中，要么在Transformers库中构建他们的模型。所以这就是为什么Transformers库总是拥GPT2、BERT这些SOTA的 NLP模型的原因。



## 商业模式business model

开源软件要经历三个阶段：

**1、Project-community fit：**GitHub stars, commits, pull requests，contributor

**2、Product-market fit：**downloads and usage

**3、Value-market fit:** revenue

Hugging Face 在**Project-community fit和Product-market fit**上无疑是非常成功的，但是在**Value-market fit上，**还处于探索期。

在2021年之前，公司的收入为0。公司CEO没有考虑过商业化。跟2C非常类似（可能是创始人一开始想做2C的公司），一开始完全不考虑商业化，一心只为了做好开源项目和开源社区。先圈人（developer），所以这可能也是为什么各项指标增长那么快的原因。

2021年初，开始进行商业化的探索。推出了优先支持、管理私有模型并托管推理API。

客户包括：Bloomberg、Typeform。

![img](https://pic2.zhimg.com/80/v2-be2c22972458659a2733bb7c128a5645_1440w.webp)

Hugging Face目前是分为这5个档次。

解决方案包括三类

**AutoNLP：**

- 现在很多做AutoML的公司，一开始尽可能地收集相关的model，包括整合开源的model zoo、整合开源的model和复现论文等，原始model丰富度至关重要！然后在不同的model之间做高效model selection。而Hugging Face做成了modelHub for NLP之后，管理那么多model，天然的适合做AutoNLP。Model质量是从源头把控，这简直是跨界打击啊，跟一开始就做AutoML公司完全不一样的思路。Hugging Face并不是上来就说要做AutoML。而先把Model Zoo的内功修炼好。

**Inference API**

- 按量付费：AutoNLP，pay as you go
- 阶形套餐：根据是否采用GPU加速，推理的文本字符总数量限制，来设立不同的套餐。
- 与GitHub的私有仓库收费类似，Hugging Face只有企业版才有私有的Model Hub。

**Premium Support：**

- 如何根据use case微调模型？用哪些基础架构？用多少训练数据？
- 如何优化模型以获得最小延迟？包括Distillation. Compilation. Quantization. Pruning.
- 如何优化生产环境？调整 CPU 或 GPU 配置以获得最佳性能。
- 如何在 SageMaker 中使用 Transformers？模型并行、数据并行、部署等。
- 如何检测和减轻数据集和模型中的偏差？



## 发展规划

- 员工数量扩大三倍，主要在纽约和巴黎
- 完成新一轮融资后，计划将纽约和巴黎的员工人数增加三倍（包括远程职位）；从NLP领域进入到CV领域



## 个人总结与思考：

**为啥2019年Hugging Face引起了Warren的注意？**

- AI三要素，包括模型、数据和算力。算力相对独立，有芯片公司来做。数据和模型联系相对比较紧密，由软件公司来干。拥有特有数据的企业，能获得一定的数据壁垒。因为独有的数据是稀缺的，有更好的数据，一定有更好的效果。
- Deep Learning领域SOTA模型层出不穷，模型的创新太快，任何一家模型提供商很难在模型上保持持续的领先性，很难拥有壁垒。模型如何商业化是一个问题。
- 为了保证模型的领先性，AI人才相对充足的企业（例如大厂和AI独角兽等），他们内部都要不断follow最新的paper，自己复现SOTA模型，并有自己对模型的规范，造成了巨大的资源浪费。（卷起来）
- AI人才相对不足的企业，没有精力、能力和财力在内部去不断follow最新的paper，复现SOTA模型，导致无法应用最新的AI科研成果。（卷不动）

**令Warren眼前一亮的Hugging Face**

切入点：自监督的语言模型就是很好的切入点。伴随着Transformer的台风，NLP自监督预训练模型的兴起，Transformer成为了新的backbone。所有基于的LSTM和CNN的模型可能都要被重做。而transformer不但席卷了NLP领域，甚至开始渗透到CV领域。

公司方法论上：bridge the gap between scientific research and engineering。

- 运营层面：采用了community-driven的方式。抓住了任何一家公司都很难在模型上保持持续领先性的特点。为了保持模型SOTA和推动AI进步需要整个领域的专家（各大AI LAB、科研机构和高校）来共同参与，一起构建了Hugging Face社区和Transformers库，科研人员把模型集成到Transformers库或者基于Transformer库构建模型。
- 落地和易用性层面：做了一层抽象，屏蔽了tensorflow和pytorch的复杂性。Tensorflow强在工业部署能力，圈了一波偏向产业界的开发者。Pytorch强在易用性，圈了一波偏学术的开发者。Hugging Face为了同时吸引了TensorFlow和PyTorch的开发者，在TensorFlow和PyTorch之上做了一层抽象，屏蔽了不同深度学习框架的复杂性，易用性特别强，解决了科研到工程落地的鸿沟。

结果：Hugging Face的Transformers库几乎开始成为了NLP SOTA的标准，形成了平台效应，一方面科研想把模型集成到Transformers库，可以把科研成果用于实践；另一方面，工程人员可以相对低门槛地用上整理好的模型库。

同时通过transformers库(ModelHub)的引流，带动了文本数据集（DataHub）。实际上几乎成为了NLP的事实标准，初步形成了NLP界的GitHub。

近期，基于transformer的模型在视觉和音频也取得了很好的效果。现在Hugging Face招3倍的人进入视觉领域，最终的也野心必然是ML界的GitHub。

**启发**：2019年的时候，Warren兴奋地跟人介绍Hugging Face，对方给Warren的回复是：模型库的流行，不一定能做成商业化公司，SKlearn就是一个好例子，也很流行。但是看到Hugging Face的transformers库的star持续地快速增长，生态的持续繁荣，并获得了后续轮的融资。嗯，Elon Musk说得对，不要轻易地用类比，要用第一性原理去分析！要相信自己的判断!



**AI是science-driven的，但是缺少了engineering friendly**

Computer Science和software Engineering：虽然Computer Science中有Science这个词，但是其实绝大多数developer不需要看paper，不需要时时刻刻follow最新的科研进展，就可以直接用开源的工具。而机器学习是science-driven，需要follow最新的进展。

数据科学家/算法研究员偏向于算法研究，软件开发工程师偏向于工程。数据科学家/算法研究员与软件开发工程师的扯皮太多了。（DevOps诞生就是因为开发者和运维的鸿沟，MLOps就是算法研究员和工程师之间的鸿沟。）

科研人员：开源代码的目的是让利用整个领域人员的力量来共同推进科学的进步，发paper其实也是一种开源，只不过是开源idea和方法。

工程人员：目的是低门槛地快速运用SOTA的算法，达到较好的效果。



**GPT和BERT为代表的自监督学习成为趋势，是迁移学习的里程碑**

- 在Transformer出现之前，在NLP领域，CNN和RNN为主的模型层数加深就容易过拟合。Transformer之后，模型层数即使加深也不容易过拟合，NLP正式进入了真正的deep model时代。巧妙的自监督训练方式，并结合了海量的数据和强大的算力，NLP效果获得了大幅度的提升。
- Transformer替代RNN和CNN成为NLP的主流，渐渐地从文本进入其他模态，包括视觉和语音等。
- 数据并没有网络效应，而且数据边际效益可能是递减，边际成本可能是增加的。
- 以往，对一个任务的预测效果提升，可能对另一个任务并没有帮助。例如：针对某个任务，优化模型或增加训练数据来提升效果，对其他任务的效果提升并没有帮助。所以针对每个任务，需要专门优化模型，需针对性的收集该任务对应的数据。自监督模型，在海量数据上做预训练后，多个下游任务的效果都会得到大幅提升。**用商业角度的话来讲，就是降低了定制化的成本，增加了产品化的成本。而且这个产品大概率是要被超越**。有趣的现象：）



## 价值分析：

警告⚠️：Warren没有和Hugging Face的创始人聊过，也没有看过Hugging Face的BP,只看过一些新闻和在youtube上的访谈。接下来Warren就要开始一本正经的胡乱推测了。

（⚠️Warren自说自话的写出了如下的公示，如果雷同，纯属巧合）

ModelHub + DataHub = MLHub(或者说是GitHub for ML) (1)

**ModelHub for NLP** + DataHub for NLP = MLHub for NLP (2)

**ModelHub for NLP** + ModelHub for CV + … = ModelHub (3)

ModelHub + DataHub = MLHub (GitHub for Machine Learning)

Hugging Face的Transformers几乎已经成为了ModelHub for NLP的事实标准, 不过目前Transformers上的model主要还是在NLP领域。

Hugging Face通过ModelHub引流，开始带动文本Dataset，可以做成MLHub（NLP界的GitHub），可以用公式（2）表示。

Hugging Face扩招了3倍的人数，开始从NLP领域，杀入CV领域，做成全领域的基于Transformer的ModelHub，可以用公式（3）来表示。

最终Hugging Face的最大野心就是做成有全领域模型和数据的MLHub（机器学习界的GitHub），可以用公式（1）表示。

机器学习界的GitHub的价值究竟有多大呢？

软件1.0：逻辑编程：code

软件2.0：数据拟合：ML= data + code(model+other code) + computing

可以看一下软件1.0的代码托管平台。

- 2018年GitHub按照75亿美元被微软收购。
- 2020年GitLab估值已经超过60亿美元，准备上市中。

机器学习界的GitHub要真能做成，价值应该不低吧。



## 开放性的问题：

**最后抛出一个开放性的问题：为什么没有其他的Model Zoo/Model Hub没有做成一家公司？**

这个问题Warren我暂时还回答不了，**欢迎相关人士与我探讨**。但是我可以分析一下Hugging Face的几个特点。

- 作者相对较少。自监督预训练语言模型的数量相对少很多。和一般的model不同，自监督预训练语言模型训练成本高昂，大多数的科研人员是没有财力去训练的，所以作者很少，往往来自于顶级的科研机构或者大厂的科研部门。
- 语言模型关注度高。自监督预训练语言模型的对大多数任务的提升都是有帮助的，所以焦点自然就到了自监督语言模型，语言模型相对集中，关注度自然高。而下游任务相对较多，相对发散。
- 自监督的潜力巨大，是AI的一个重要方向。



------

Warren坚定地看好基于数据编程体系的创业机会，尤其是AI infra方面的创业机会。为了更好的帮助AI创业者，抱着Open Source Research as a service的心态，我开源自己的研究成果，发起了SSAIS项目。

[https://github.com/WarrenWen666/AI-Software-Startups/blob/main/SSAIS_MLOps.mdgithub.com/WarrenWen666/AI-Software-Startups/blob/main/SSAIS_MLOps.md](https://link.zhihu.com/?target=https%3A//github.com/WarrenWen666/AI-Software-Startups/blob/main/SSAIS_MLOps.md)

[GitHub - WarrenWen666/AI-Software-Startups: A Survey of AI startupsgithub.com/WarrenWen666/AI-Software-Startups](https://link.zhihu.com/?target=https%3A//github.com/WarrenWen666/AI-Software-Startups)

欢迎AI领域的从业人员，与我交流，进行头脑风暴，共同探讨AI的商业化发展方向。



Warren Wen（[耀途资本](https://link.zhihu.com/?target=https%3A//www.glory-ventures.com/)，glory-ventures）

联系方式：[warrenwen@glory-ventures.com](mailto:warrenwen@glory-ventures.com)



参考资料：

- [https://huggingface.co/](https://link.zhihu.com/?target=https%3A//huggingface.co/)
- [https://github.com/huggingface](https://link.zhihu.com/?target=https%3A//github.com/huggingface)
- [https://www.youtube.com/watch?v=bRyTEdyDp-c](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DbRyTEdyDp-c)
- [https://wandb.ai/wandb_fc/gradient-dissent/reports/Cl-ment-Delangue-CEO-of-Hugging-Face-on-the-power-of-the-open-source-community--Vmlldzo3NjcyMDg?galleryTag=](https://link.zhihu.com/?target=https%3A//wandb.ai/wandb_fc/gradient-dissent/reports/Cl-ment-Delangue-CEO-of-Hugging-Face-on-the-power-of-the-open-source-community--Vmlldzo3NjcyMDg%3FgalleryTag%3D)
- [https://www.youtube.com/watch?v=0_zS-wGSXCo](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3D0_zS-wGSXCo)
- [https://techcrunch.com/2019/12/17/hugging-face-raises-15-million-to-build-the-definitive-natural-language-processing-library/](https://link.zhihu.com/?target=https%3A//techcrunch.com/2019/12/17/hugging-face-raises-15-million-to-build-the-definitive-natural-language-processing-library/https%3A/techcrunch.com/2021/03/11/hugging-face-raises-40-million-for-its-natural-language-processing-library/)
- [https://techcrunch.com/2021/03/11/hugging-face-raises-40-million-for-its-natural-language-processing-library/](https://link.zhihu.com/?target=https%3A//techcrunch.com/2019/12/17/hugging-face-raises-15-million-to-build-the-definitive-natural-language-processing-library/https%3A/techcrunch.com/2021/03/11/hugging-face-raises-40-million-for-its-natural-language-processing-library/)
- [https://venturebeat.com/2019/12/17/hugging-face-raises-15-million-to-build-open-source-community-for-cutting-edge-conversational-ai/](https://link.zhihu.com/?target=https%3A//venturebeat.com/2019/12/17/hugging-face-raises-15-million-to-build-open-source-community-for-cutting-edge-conversational-ai/)
- [https://venturebeat.com/2021/03/11/hugging-face-triples-investment-in-open-source-machine-learning-models/](https://link.zhihu.com/?target=https%3A//venturebeat.com/2021/03/11/hugging-face-triples-investment-in-open-source-machine-learning-models/)

编辑于 2021-10-07 12:10



https://zhuanlan.zhihu.com/p/411174344