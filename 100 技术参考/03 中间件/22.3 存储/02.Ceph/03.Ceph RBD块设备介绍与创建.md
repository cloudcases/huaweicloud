# ③ Ceph—RBD块设备介绍与创建

[Linux丶晨星](https://www.jianshu.com/u/4110dc13fdb2)

2020.05.07 11:27:24字数 1,047阅读 1,260

# 一、RBD介绍

> RBD即RADOS Block Device的简称，RBD块存储是最稳定且最常用的存储类型。RBD块设备类似磁盘可以被挂载。 RBD块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在Ceph集群的多个OSD中。如下是对Ceph RBD的理解。

- RBD 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用；
- resizable：这个块可大可小；
- data striped：这个块在Ceph里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下；
- thin-provisioned：精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。举例：你有一个 32G 的 U盘，存了一个2G的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间 ；

> 块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。
>
> > ceph可以通过内核模块和librbd库提供块设备支持。客户端可以通过内核模块挂在rbd使用，客户端使用rbd块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；客户应用也可以通过librbd使用ceph块，典型的是云平台的块存储服务（如下图），云平台可以使用rbd作为云的存储后端提供镜像存储、volume块或者客户的系统引导盘等。

使用场景：

- 云平台（OpenStack做为云的存储后端提供镜像存储）
- K8s容器
- map成块设备直接使用
- ISCIS，安装Ceph客户端

# 二、RBD常用命令

| 命令           | 功能                      |
| -------------- | ------------------------- |
| rbd create     | 创建块设备映像            |
| rbd ls         | 列出 rbd 存储池中的块设备 |
| rbd info       | 查看块设备信息            |
| rbd diff       | 可以统计 rbd 使用量       |
| rbd map        | 映射块设备                |
| rbd showmapped | 查看已映射块设备          |
| rbd remove     | 删除块设备                |
| rbd resize     | 更改块设备的大小          |

# 三、RBD配置操作

## 3.1 RBD挂载到操作系统

1、创建rbd使用的pool

```shell
#32 32: PG数和PGP数(PG内包含的对象);PG会随着容量的增加也会动态的扩容；生产上需要提前规划好数量
ceph osd pool create rbd  32 32
#给rbd使用的pool标识成rbd
ceph osd pool application enable rbd rbd 

#查看rbd的pool
[root@cephnode01 my-cluster]# ceph osd pool ls detail
pool 5 'rbd' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode warn last_change 68 flags hashpspool stripe_width 0
```

2、创建一个10G的块设备

```shell
rbd create --size 10240 image01 
```

3、查看块设备

```shell
rbd ls
rbd info image01
rados -p rbd ls --all
```

4、禁用当前系统内核不支持的feature

```powershell
rbd feature disable image01 exclusive-lock, object-map, fast-diff, deep-flatten
```

5、将块设备映射到系统内核

```shell
rbd map image01 
rbd showmapped
```

6、格式化块设备镜像

```shell
mkfs.xfs /dev/rbd0
```

7、mount到本地

```shell
mount /dev/rbd0 /mnt
```

8、创建文件并查看

```shell
[root@cephnode01 my-cluster]# touch /mnt/{a..g}lcx.txt
[root@cephnode01 my-cluster]# rados -p rbd ls --all
    rbd_data.28209e4598ff7.0000000000000500
    rbd_id.image01
    rbd_header.28209e4598ff7
    rbd_data.28209e4598ff7.0000000000000820
    rbd_directory
    rbd_data.28209e4598ff7.0000000000000460
    rbd_data.28209e4598ff7.00000000000006e0
    rbd_info
    rbd_data.28209e4598ff7.00000000000000a0
    rbd_data.28209e4598ff7.00000000000009ff
    rbd_data.28209e4598ff7.00000000000005a0
    rbd_data.28209e4598ff7.0000000000000000
    rbd_data.28209e4598ff7.0000000000000780
...
```

9、取消块设备和内核映射

```shell
umount /mnt
rbd unmap image01 
```

10、删除RBD块设备

```shell
rbd rm image01
```

> **如果想在其他机器上使用ceph块设备需要下载ceph-common客户端**

## 3.2 快照配置

> **快照会占用集群很大的空间，建议对重要的块定期拍摄快照；或者用其他方法进行备份**

1、创建快照

```shell
rbd create --size 10240 image02 
#前面是rbd的名称，后面是快照的名称
rbd snap create image02@image02_snap01
```

2、列出创建的快照

```shell
rbd snap list image02
或
rbd ls -l
```

3、查看快照详细信息

```shell
rbd info image02@image02_snap01
```

4、克隆快照（快照必须处于被保护状态才能被克隆;就是不能对快照做任何的操作）

```shell
#创建一个克隆的pool
ceph osd pool create kube 16 16
ceph osd pool application enable kube kube
#克隆
rbd snap protect image02@image02_snap01
rbd clone rbd/image02@image02_snap01 kube/image02_clone01

#查看kube的pool中的快照
rbd ls -p kube
image02_clone01
```

5、查看快照的命令`children`（可以查看哪个是子快照）

```shell
rbd children image02
```

6、去掉快照的`parent`(去掉克隆的块关系;就是成为了独立的快照)

```shell
rbd flatten kube/image02_clone01
```

7、恢复快照

```shell
rbd snap rollback image02@image02_snap01
```

8、删除快照

```shell
#先取消保护
rbd snap unprotect image02@image02_snap01
rbd snap remove image02@image02_snap01
```

## 3.3 导出导入RBD镜像

1、导出RBD镜像

```shell
rbd export image02 /tmp/image02
ll -h /tmp/image02 
-rw-r--r-- 1 root root 10G May  7 11:36 /tmp/image02
```

2、导入RBD镜像

```shell
#可以删除掉原理的镜像 
rbd remove image02
rbd import /tmp/image02 rbd/image02 --image-format 2 
```

3、查看导入的rbd

```shell
rbd ls
rbd info image02
```

**更新所有节点的配置文件的命令**

```undefined
ceph-deploy --overwrite-conf config push cephnode01 cephnode02 cephnode03 
```

启动所有守护进程

```css
systemctl start ceph.target
```

## 3.4 扩容

> 扩容对数据没有影响

1、使用命令`rbd resize`进行扩容

```shell
#image02为10G
[root@cephnode01 my-cluster]# rbd ls
image02
```

2、扩容到20G

```shell
rbd --image image02 resize --size 20480
[root@cephnode01 my-cluster]# rbd info image02
rbd image 'image02':
    size 20 GiB in 5120 objects     #现在扩容到20G了
    order 22 (4 MiB objects)
    snapshot_count: 0
    id: 28338cfe50192
    block_name_prefix: rbd_data.28338cfe50192
    format: 2
    features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
    op_features: 
    flags: 
    create_timestamp: Thu May  7 11:37:51 2020
    access_timestamp: Thu May  7 11:37:51 2020
    modify_timestamp: Thu May  7 11:37:51 2020
```

3、缩容

> XFS文件系统不支持缩小

```shell
# --allow-shrink参数
rbd --image image02 resize --size  10240 --allow-shrink 
```



https://www.jianshu.com/p/712e58d36a77