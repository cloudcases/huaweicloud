# 部署Ceph分布式存储，迁移网站数据到ceph集群

[一个小运维](https://www.jianshu.com/u/e13f1873e780)

2021.05.29 09:06:28字数 761阅读 408

- 使用三台服务器部署Ceph分布式存储，实现Ceph文件系统共享，将网站数据从NFS迁移到Ceph存储

实验拓扑如下图所示，做具体实验前请先配置好环境：

![img](https://upload-images.jianshu.io/upload_images/26326118-50f88adb2b56feac.png?imageMogr2/auto-orient/strip|imageView2/2/w/501/format/webp)

image

主机配置如下表所示：

![img](https://upload-images.jianshu.io/upload_images/26326118-d7009207f3e25b4f.png?imageMogr2/auto-orient/strip|imageView2/2/w/572/format/webp)

##### 一：准备环境

1）将3台虚拟机全部关机,添加光盘和磁盘，右击虚拟机，选【设置】---【添加】---【CD|DVD驱动器】--【完成】，选择镜像文件 ceph10.iso，所有3台ceph服务器都添加2块20G磁盘。

启动所有虚拟机后，查看磁盘情况：

```undefined
lsblk
```

所有主机设置防火墙和SELinux：

```csharp
firewall-cmd --set-default-zone=trusted
sed -i '/SELINUX/s/enforcing/permissive/' /etc/selinux/config
setenforce 0
```

2）所有主机挂载ceph光盘和系统光盘（根据实际情况挂载，不用照抄）

```ruby
[root@node1 ~]# umount /dev/sr0
[root@node1 ~]# umount /dev/sr1
[root@node1 ~]# mkdir /ceph
[root@node1 ~]# vim /etc/fstab
/dev/sr0 /ceph     iso9660   defaults   0  0
/dev/sr1 /media    iso9660   defaults   0  0
[root@node1 ~]# mount -a
```

3）在node1配置SSH密钥，让node1可用无密码连接node1,node2,node3

```bash
ssh-keygen  -f /root/.ssh/id_rsa  -N  ''
#-f后面跟密钥文件的名称（创建密钥到哪个文件）
#-N  ''设置密钥的密码为空（不要给密钥配置密码）

[root@node1 ~]# for i in   41  42  43
do
ssh-copy-id  192.168.2.$i
done
#通过ssh-copy-id将密钥传递给node1，node2，node3
```

4)修改/etc/hosts域名解析记录（不要删除原文件的数据），同步给所有ceph节点。

```ruby
vim /etc/hosts      #修改文件，手动添加如下内容（不要删除原文件的数据）
192.168.2.41    node1
192.168.2.42     node2
192.168.2.43    node3

for i in 41 42 43
do
scp /etc/hosts 192.168.2.$i:/etc
done
```

5）为所有ceph节点配置yum源，并将配置同步给所有节点

提示：前面已经将ceph的光盘挂载到/ceph目录。

```csharp
cat /etc/yum.repos.d/ceph.repo
[mon]
name=mon
baseurl=file:///ceph/MON
gpgcheck=0
[osd]
name=osd
baseurl=file:///ceph/OSD
gpgcheck=0
[tools]
name=tools
baseurl=file:///ceph/Tools
gpgcheck=0

[root@node1 ~]# yum repolist                #验证YUM源软件数量

[root@node1 ~]# for i in 41 42 43
do
scp /etc/yum.repos.d/ceph.repo 192.168.2.$i:/etc/yum.repos.d/
done
```

6）配置NTP服务器同步时间。

node1做服务端：

```bash
vim /etc/chrony.conf
allow 192.168.2.0/24        #修改26行
local stratum 10            #修改29行(去注释即可)

systemctl restart chronyd
```

node2和node3做客户端：

```csharp
[root@node2 ~]# vim /etc/chrony.conf
server 192.168.2.41   iburst              #配置文件第二行，手动加入该行内容
[root@node2 ~]# systemctl restart chronyd
[root@node2 ~]# chronyc sources -v

[root@node3 ~]# vim /etc/chrony.conf
server 192.168.2.41   iburst              #配置文件第二行，手动加入该行内容
[root@node3 ~]# systemctl restart chronyd
[root@node3 ~]# chronyc sources -v
```

##### 二：部署ceph集群

1）给node1主机安装ceph-deploy，创建工作目录，初始化配置文件。

```csharp
[root@node1 ~]# yum -y install ceph-deploy
[root@node1 ~]# mkdir ceph-cluster
[root@node1 ~]# cd ceph-cluster
```

2）给所有ceph节点安装ceph相关软件包



```bash
[root@node1 ceph-cluster]# for i in node1 node2 node3
do
ssh $i "yum -y install ceph-mon ceph-osd ceph-mds"
done
```

3）初始化mon服务

```csharp
[root@node1 ceph-cluster]# ceph-deploy new node1 node2 node3               #生成ceph配置文件
[root@node1 ceph-cluster]# ceph-deploy mon create-initial
\#拷贝ceph配置文件给node1,node2,node3，启动所有主机的mon服务
[root@node1 ceph-cluster]# ceph -s                    #查看状态（此时失败是正常的）
cluster 9f3e04b8-7dbb-43da-abe6-b9e3f5e46d2e
health HEALTH_ERR
monmap e2: 3 mons at
{node1=192.168.2.41:6789/0,node2=192.168.2.42:6789/0,node3=192.168.2.43:6789/0}

osdmap e45: 0 osds: 0 up, 0 in
```

4）使用ceph-deploy工具初始化数据磁盘（仅node1操作），硬盘名称根据实际情况填写，不能照抄。

```css
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node1:sdb  node1:sdc    
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node2:sdb  node2:sdc
[root@node1 ceph-cluster]# ceph-deploy disk  zap  node3:sdb  node3:sdc
```

5）初始化OSD集群，磁盘名称根据实际情况填写。

```css
[root@node1 ceph-cluster]# ceph-deploy osd create  node1:sdb  node1:sdc  
#每个磁盘都会被自动分成两个分区；一个固定5G大小；一个为剩余所有容量
#5G分区为Journal缓存；剩余所有空间为数据盘。
[root@node1 ceph-cluster]# ceph-deploy osd create  node2:sdb  node2:sdc
[root@node1 ceph-cluster]# ceph-deploy osd create  node3:sdb  node3:sdc 

[root@node1 ceph-cluster]# ceph -s                 #查看集群状态，状态为OK
```

##### 三：部署ceph文件系统

1）启动mds服务（可以在node1或node2或node3启动，也可以在多台主机启动mds）

```csharp
[root@node1 ceph-cluster]# ceph-deploy mds create node3
```

2）创建存储池（文件系统由inode和block组成）

```csharp
[root@node1 ceph-cluster]# ceph osd pool create cephfs_data 64
[root@node1 ceph-cluster]# ceph osd pool create cephfs_metadata 64
[root@node1 ceph-cluster]# ceph osd lspools      #查看共享池
0 rbd,1 cephfs_data,2 cephfs_metadata
```

3）创建文件系统

```csharp
[root@node1 ceph-cluster]# ceph fs new myfs1 cephfs_metadata cephfs_data
[root@node1 ceph-cluster]# ceph fs ls
name: myfs1, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
```

##### 四：迁移网站数据到ceph集群

1）卸载web1，web2，web3的NFS共享。

暂停服务防止有人实时读写文件：

```csharp
[root@web1 ~]# /usr/local/nginx/sbin/nginx -s stop
[root@web2 ~]# /usr/local/nginx/sbin/nginx -s stop
[root@web3 ~]# /usr/local/nginx/sbin/nginx -s stop
[root@web1 ~]# umount /usr/local/nginx/html
[root@web2 ~]# umount /usr/local/nginx/html
[root@web3 ~]# umount /usr/local/nginx/html
[root@web1 ~]# vim /etc/fstab
#192.168.2.31:/web_share/html /usr/local/nginx/html/ nfs defaults 0 0
[root@web2 ~]# vim /etc/fstab
#192.168.2.31:/web_share/html /usr/local/nginx/html/ nfs defaults 0 0
[root@web3 ~]# vim /etc/fstab
#192.168.2.31:/web_share/html /usr/local/nginx/html/ nfs defaults 0 0
```

2）web服务器永久挂载Ceph文件系统（web1、web2、web3都需要操作）。

在任意ceph节点，如node1查看ceph账户与密码:

```csharp
[root@node1 ~]# cat /etc/ceph/ceph.client.admin.keyring 
[client.admin]
key = AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==

/etc/rc.local是开机启动脚本，任何命令放在该文件中都是开机自启。
```

ceph-common是ceph的客户端软件：

```csharp
[root@web1 ~]# yum -y install ceph-common
[root@web2 ~]# yum -y install ceph-common
[root@web3 ~]# yum -y install ceph-common

[root@web1 ~]# mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ -o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==
[root@web1 ~]# echo 'mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ -o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==' >> /etc/rc.local 
[root@web1 ~]# chmod +x /etc/rc.local

[root@web2 ~]# mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ -o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==
[root@web2 ~]# echo 'mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ -o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==' >> /etc/rc.local 
[root@web2 ~]# chmod +x /etc/rc.local

[root@web3 ~]#  mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ -o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==
[root@web3 ~]# echo 'mount -t ceph 192.168.2.41:6789:/ /usr/local/nginx/html/ -o name=admin,secret=AQA0KtlcRGz5JxAA/K0AD/uNuLI1RqPsNGC7zg==' >> /etc/rc.local 
[root@web3 ~]# chmod +x /etc/rc.local
```

另一种解决方案，还可以通过fstab实现永久挂载。

提示：如果希望使用fstab实现永久挂载，客户端需要额外安装libcephfs1软件包。

```ruby
[root@web1 ~]# yum -y install libcephfs1
[root@web1 ~]# vim /etc/fstab
… …
192.168.2.41:6789:/ /usr/local/nginx/html/    ceph    defaults,_netdev,name=admin,secret=AQCVcu9cWXkgKhAAWSa7qCFnFVbNCTB2DwGIOA== 0 0
```

第三种挂载方案：对于高可用的问题，可以在mount时同时写入多个IP。

```csharp
临时命令：

[root@web1 ~]# mount -t ceph  192.168.2.41:6789,192.168.2.42:6789,192.168.2.43:6789:/ /usr/local/nginx/html  -o name=admin,secret=密钥
```

永久修改：

```ruby
[root@web1 ~]# vim /etc/fstab
192.168.2.41:6789,192.168.2.42:6789,192.168.2.43:6789:/ /usr/local/nginx/html/ ceph defaults,_netdev,name=admin,secret=密钥 0 0
```

3)迁移NFS服务器中的数据到Ceph存储

登陆NFS服务器备份数据，将备份数据拷贝给web1或web2或web3，tar备份数据时注意使用-p选项保留文件权限。

```ruby
[root@nfs ~]# cd /web_share/html/
[root@nfs html]# tar -czpf /root/html.tar.gz ./*
[[root@nfs](mailto:root@nfs) html]# scp /root/html.tar.gz 192.168.2.11:/usr/local/nginx/html/
```

登陆web1将数据恢复到Ceph共享目录

```css
[root@web1 html]# tar -xf html.tar.gz
[root@web1 html]# rm -rf html.tar.gz
```

4）恢复web服务

```csharp
[root@web1 ~]# /usr/local/nginx/sbin/nginx
[root@web2 ~]# /usr/local/nginx/sbin/nginx
[root@web3 ~]# /usr/local/nginx/sbin/nginx
```



https://www.jianshu.com/p/04b64cd68866