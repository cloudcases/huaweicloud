# Ceph分布式存储原理

[_二师兄_](https://www.jianshu.com/u/2f87a83ea8c8)

2020.03.20 19:54:23字数 524阅读 385

## Ceph基本介绍

### 1. 定义

```swift
Ceph是一个分布式存储系统(由c++编写完成，提供软件定义、统一存储解决方案)，根据存储类型可分为块存储、对象存储和文件存储；
```

### 2. 特点

```xml
  <1> Ceph支持对象存储、块存储和文件存储服务，故称为统一存储。
  <2> 采用CRUSH算法，数据分布均衡，并行度高，不需要维护固定的元数据结构；
  <3> 数据具有强一致，确保所有副本写入完成才返回确认，适合读多写少场景；
  <4> 去中心化，MDS之间地位相同，无固定的中心节点
```

Ceph存在一些缺点
<1> 去中心化的分布式解决方案，需要提前做好规划设计，对技术团队的要求能力比较高。
<2> Ceph扩容时，由于其数据分布均衡的特性，会导致整个存储系统性能的下降。

Ceph相比于其他存储方案的优势

```xml
<1> CRUSH算法：Crush算法是ceph的两大创新之一，简单来说，Ceph摒弃了传统的集中式存储元数据寻址的方案，转而使用CRUSH算法完成数据的寻址操作。CRUSH在一致性哈希基础上很好的考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。Crush算法有相当强大的扩展性，理论上支持数千个存储节点。

<2> 高可用：Ceph中的数据副本数量可以由管理员自行定义，并可以通过CRUSH算法指定副本的物理存储位置以分隔故障域，支持数据强一致性； Ceph可以忍受多种故障场景并自动尝试并行修复；Ceph支持多份强一致性副本，副本能够垮主机、机架、机房、数据中心存放。所以安全可靠。Ceph存储节点可以自管理、自动修复。无单点故障，容错性强。

<3> 高性能：因为是多个副本，因此在读写操作时候能够做到高度并行化。理论上，节点越多，整个集群的IOPS和吞吐量越高。另外一点Ceph客户端读写数据直接与存储设备(osd) 交互。在块存储和对象存储中无需元数据服务器。

<4> 高扩展性：Ceph不同于Swift，客户端所有的读写操作都要经过代理节点。一旦集群并发量增大时，代理节点很容易成为单点瓶颈。Ceph本身并没有主控节点，扩展起来比较容易，并且理论上，它的性能会随着磁盘数量的增加而线性增长。Ceph扩容方便、容量大。能够管理上千台服务器、EB级的容量。

<5> 特性丰富：Ceph支持三种调用接口：对象存储，块存储，文件系统挂载。三种方式可以一同使用。在国内一些公司的云环境中，通常会采用Ceph作为openstack的唯一后端存储来提升数据转发效率。Ceph是统一存储，虽然它底层是一个分布式文件系统，但由于在上层开发了支持对象和块的接口，所以在开源存储软件中，优势很明显。
```

### 3 Ceph主要架构

![img](https://upload-images.jianshu.io/upload_images/18500080-50f014e53929cf29.png?imageMogr2/auto-orient/strip|imageView2/2/w/674/format/webp)

image.png

a) ceph最底部为RADOS对象存储系统，主要由OSD和Monitor两部分组成，最终的数据存储在OSD中；
b) RADOS上层是librados（一个工具库），它允许应用程序通过访问librados来和rados系统进行交互；
c) 在librados上面就是针对各个类型跟ceph集群交互的插件



```swift
     1. ceph集群允许应用程序和他直接交互，目前支持的语言有 c++、java、python、ruby、php
     2.  若是对象存储类型，必须经过 radosgw 即rados 对象网关，通过restfulAPI 形式与ceph交互，目前radosgw兼容 openstack swift api，Amzon S3 API 以及ceph原生admin API
     3.  块存储ceph内部就是通过工具包librbd与应用程序进行交互
     4.  对于文件存储，ceph提供工具库 libcephfs/posix相关库
```

### 4. Ceph功能模块

![img](https://upload-images.jianshu.io/upload_images/18500080-af157c8fc978654d.png?imageMogr2/auto-orient/strip|imageView2/2/w/631/format/webp)

image.png



Ceph 的核心组件包括Monitor监控服务，OSD存储服务，MDS元数据服务以及客户端Client，各个核心组件功能如下：

- Ceph OSDs： ceph OSD 守护进程，主要功能是存储数据，处理数据的复制、恢复、回填和再均衡，并通过检查其他OSD之间的心跳向Monitor提供监控信息。
- Monitors: ceph monitor负责整个集群的监控，维护集群的健康状态，包括监视图、OSD图、归置(PG)图、和CRUSH图。
- MDS: Ceph元数据服务为ceph文件系统存储元数据，管理目录结构
- Client: 主要负责存储协议的接入和节点负载均衡

### 5. Ceph数据写入流程

<1> 数据通过负载均衡获得节点IP地址；
<2> 然后通过块、文件、对象协议将数据传输到对应节点
<3> 数据被分割成最大为4m对象并取得唯一对象ID
<4> 然后通过HASH算法将对象ID分配到不同的PG
<5> 最后不同的PG再通过CRUSH算法存到不同的OSD上



https://www.jianshu.com/p/e9e0722e82a8