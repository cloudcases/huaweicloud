# Ceph分布式存储

[![img](https://upload.jianshu.io/users/upload_avatars/11706687/205bd6ed-8a52-4994-8b52-c1e7bd8635f5.jpg?imageMogr2/auto-orient/strip|imageView2/1/w/96/h/96/format/webp)](https://www.jianshu.com/u/30d75087236a)

[杰森斯坦sen](https://www.jianshu.com/u/30d75087236a)关注

0.0872019.08.12 11:39:13字数 2,498阅读 22,791

### Ceph

Ceph是一个可靠、自动重均衡、自动恢复的分布式存储系统，根据场景划分可以将Ceph分为三大块，分别是对象存储、块设备和文件系统服务。块设备存储是Ceph的强项。

Ceph的主要优点是分布式存储，在存储每一个数据时，都会通过计算得出该数据存储的位置，尽量将数据分布均衡，不存在传统的单点故障的问题，可以水平扩展。

### Ceph架构

![img](https://upload-images.jianshu.io/upload_images/11706687-a54c641eb6c5ee83.png?imageMogr2/auto-orient/strip|imageView2/2/w/693/format/webp)

Ceph架构

RADOS自身是一个完整的分布式对象存储系统，它具有可靠、智能、分布式等特性，Ceph的高可靠、高可拓展、高性能、高自动化都是由这一层来提供的，用户数据的存储最终也都是通过这一层来进行存储的，RADOS可以说就是Ceph的核心组件。

RADOS系统主要由两部分组成，分别是OSD和Monitor。

基于RADOS层的上一层是LIBRADOS，LIBRADOS是一个库，它允许应用程序通过访问该库来与RADOS系统进行交互，支持多种编程语言，比如C、C++、Python等。

基于LIBRADOS层开发的又可以看到有三层，分别是RADOSGW、RBD和CEPH FS。

RADOSGW：RADOSGW是一套基于当前流行的RESTFUL协议的网关，并且兼容S3和Swift。
RBD：RBD通过Linux内核客户端和QEMU/KVM驱动来提供一个分布式的块设备。
CEPH FS：CEPH FS通过Linux内核客户端和FUSE来提供一个兼容POSIX的文件系统。

### Ceph核心组件RADOS

RADOS系统主要由两部分组成，分别是OSD和Monitor。

Ceph OSD：OSD的英文全称是Object Storage Device，它的主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor。一般情况下一块硬盘对应一个OSD，由OSD来对硬盘存储进行管理，当然一个分区也可以成为一个OSD。

Ceph Monitor：由该英文名字我们可以知道它是一个监视器，负责监视Ceph集群，维护Ceph集群的健康状态，同时维护着Ceph集群中的各种Map图，比如OSD Map、Monitor Map、PG Map和CRUSH Map，这些Map统称为Cluster Map，Cluster Map是RADOS的关键数据结构，管理集群中的所有成员、关系、属性等信息以及数据的分发，比如当用户需要存储数据到Ceph集群时，OSD需要先通过Monitor获取最新的Map图，然后根据Map图和object id等计算出数据最终存储的位置。

![img](https://upload-images.jianshu.io/upload_images/11706687-9cd5e13814c9398e.png?imageMogr2/auto-orient/strip|imageView2/2/w/410/format/webp)

副本

为保证高可用性， Ceph 存储集群应该保存两份以上的对象副本。Ceph OSD 守护进程自动在其它 Ceph 节点上创建对象副本来确保数据安全和高可用性。

Ceph 监视器维护着集群运行图的主副本。为保证高可用性，监视器也实现了集群化。一个监视器集群确保了当某个监视器失效时的高可用性。

### Ceph数据分布算法

Ceph是为大规模分布式存储而设计的，数据分布算法必须能够满足在大规模的集群下数据依然能够快速的准确的计算存放位置，同时能够在硬件故障或扩展硬件设备时做到尽可能小的数据迁移，Ceph的CRUSH算法就是精心为这些特性设计的。

在说明CRUSH算法的基本原理之前，先介绍几个概念和它们之间的关系。

**Object：** 当用户要将数据存储到Ceph集群时，存储数据都会被分割成多个Object，每个Object都有一个object id，每个Object的大小是可以设置的，默认是4MB，Object可以看成是Ceph存储的最小存储单元。

![img](https://upload-images.jianshu.io/upload_images/11706687-337b1defcd9147d1.png?imageMogr2/auto-orient/strip|imageView2/2/w/390/format/webp)

Object

**PG：**由于Object的数量很多，所以Ceph引入了PG的概念用于管理Object，每个Object最后都会通过CRUSH计算映射到某个PG中，一个PG可以包含多个Object。

**PG与OSD的关系：**PG也需要通过CRUSH计算映射到OSD中去存储，如果是二副本的，则每个PG都会映射到二个OSD，比如[OSD#1,OSD#2]，那么OSD#1是存放该PG的主副本，OSD#2是存放该PG的从副本，保证了数据的冗余。

把对象映射到归置组在 OSD 和客户端间创建了一个间接层。由于 Ceph 集群必须能增大或缩小、并动态地重均衡。如果让客户端“知道”哪个 OSD 有哪个对象，就会导致客户端和 OSD 紧耦合；相反， CRUSH 算法把对象映射到归置组、然后再把各归置组映射到一或多个 OSD ，这一间接层可以让 Ceph 在 OSD 守护进程和底层设备上线时动态地重均衡。下列图表描述了 CRUSH 如何将对象映射到归置组、再把归置组映射到 OSD 。

![img](https://upload-images.jianshu.io/upload_images/11706687-4fba2b2014e147ed.png?imageMogr2/auto-orient/strip|imageView2/2/w/600/format/webp)

PG 映射到 OSD

**PG和PGP的关系：**pg是用来存放object的，pgp相当于是pg存放osd的一种排列组合，我举个例子，比如有3个osd，osd.1、osd.2和osd.3，副本数是2，如果pgp的数目为1，那么pg存放的osd组合就只有一种，可能是[osd.1,osd.2]，那么所有的pg主从副本分别存放到osd.1和osd.2，如果pgp设为2，那么其osd组合可以两种，可能是[osd.1,osd.2]和[osd.1,osd.3]，是不是很像我们高中数学学过的排列组合，pgp就是代表这个意思。**一般来说应该将pg和pgp的数量设置为相等**。

![img](https://upload-images.jianshu.io/upload_images/11706687-69984801fd28704b.png?imageMogr2/auto-orient/strip|imageView2/2/w/614/format/webp)

object、pg、pool、osd、存储磁盘的关系

本质上CRUSH算法是根据存储设备的**权重**来计算数据对象的分布的，权重的设计可以根据该磁盘的容量和读写速度来设置，比如根据容量大小可以将1T的硬盘设备权重设为1，2T的就设为2，在计算过程中，CRUSH是根据Cluster Map、数据分布策略和一个随机数共同决定数组最终的存储位置的。

**Cluster Map**里的内容信息包括存储集群中可用的存储资源及其相互之间的空间层次关系，比如集群中有多少个支架，每个支架中有多少个服务器，每个服务器有多少块磁盘用以OSD等。

**数据分布策略**是指可以通过Ceph管理者通过配置信息指定数据分布的一些特点，比如管理者配置的故障域是Host，也就意味着当有一台Host起不来时，数据能够不丢失，CRUSH可以通过将每个pg的主从副本分别存放在不同Host的OSD上即可达到，不单单可以指定Host，还可以指定机架等故障域，除了故障域，还有选择数据冗余的方式，比如副本数或纠删码。

![img](https://upload-images.jianshu.io/upload_images/2099201-f0f7321a9e37361f.png?imageMogr2/auto-orient/strip|imageView2/2/w/865/format/webp)

层级化的Cluster Map

下面这个式子简单的表明CRUSH的计算表达式：



```css
CRUSH(X)  -> (osd.1,osd.2.....osd.n)
```

式子中的X就是一个随机数。

下面通过一个计算PG ID的示例来看CRUSH的一个计算过程：

（1）Client输入Pool ID和对象ID；

（2）CRUSH获得对象ID并对其进行Hash运算；

（3）CRUSH计算OSD的个数，Hash取模获得PG的ID，比如0x48；

（4）CRUSH取得该Pool的ID，比如是1；

（5）CRUSH预先考虑到Pool ID相同的PG ID，比如1.48。

### 一个栗子

先创建一个名为testpool包含6个PG和6个PGP的存储池。



```undefined
ceph osd pool create testpool 6 6
```

通过写数据后我们查看下pg的分布情况：



```dart
ceph pg dump pgs | grep ^1 | awk '{print $1,$2,$15}'

dumped pgs in format plain
1.1 75 [3,6,0]
1.0 83 [7,0,6]
1.3 144 [4,1,2]
1.2 146 [7,4,1]
1.5 86 [4,6,3]
1.4 80 [3,0,4]
```

第1列为pg的id，第2列为该pg所存储的对象数目，第3列为该pg所在的osd

我们扩大PG再看看：



```bash
ceph osd pool set testpool pg_num 12
```

再次用上面的命令查询分布情况：



```dart
ceph pg dump pgs | grep ^1 | awk '{print $1,$2,$15}'

dumped pgs in format plain
1.1 37 [3,6,0]
1.9 38 [3,6,0]
1.0 41 [7,0,6]
1.8 42 [7,0,6]
1.3 48 [4,1,2]
1.b 48 [4,1,2]
1.7 48 [4,1,2]
1.2 48 [7,4,1]
1.6 49 [7,4,1]
1.a 49 [7,4,1]
1.5 86 [4,6,3]
1.4 80 [3,0,4]
```

我们可以看到pg的数量增加到12个了，pg1.1的对象数量本来是75的，现在是37个，可以看到它把对象数分给新增的pg1.9了，刚好是38，加起来是75，而且可以看到pg1.1和pg1.9的osd盘是一样的。而且可以看到osd盘的组合还是那6种。

我们增加pgp的数量来看下，使用命令：



```bash
ceph osd pool set testpool pgp_num 12
```

再看下：



```dart
ceph pg dump pgs | grep ^1 | awk '{print $1,$2,$15}'

dumped pgs in format plain
1.a 49 [1,2,6]
1.b 48 [1,6,2]
1.1 37 [3,6,0]
1.0 41 [7,0,6]
1.3 48 [4,1,2]
1.2 48 [7,4,1]
1.5 86 [4,6,3]
1.4 80 [3,0,4]
1.7 48 [1,6,0]
1.6 49 [3,6,7]
1.9 38 [1,4,2]
1.8 42 [1,2,3]
```

再看pg1.1和pg1.9，可以看到pg1.9不在[3,6,0]上，而在[1,4,2]上了，该组合是新加的，可以知道增加pgp_num其实是增加了osd盘的组合。

通过实验总结：
（1）PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数
（2）PG的增加会引起PG内的数据进行分裂，分裂相同的OSD上新生成的PG当中
（3）PGP的增加会引起部分PG的分布进行变化，但是不会引起PG内对象的变动

### Reference

[Ceph基础知识和基础架构认识](https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cnblogs.com%2Fluohaixian%2Fp%2F8087591.html)
[Ceph体系结构](https://links.jianshu.com/go?to=http%3A%2F%2Fdocs.ceph.org.cn%2Farchitecture%2F)
[Ceph介绍及原理架构分享](https://www.jianshu.com/p/cc3ece850433)



https://www.jianshu.com/p/d9f2eb6ad2ba